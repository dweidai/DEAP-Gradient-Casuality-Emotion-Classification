{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For AROUSAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/the-4-convolutional-neural-network-models-that-can-classify-your-fashion-images-9fe7f3e5399d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"/Users/apple/Desktop/eeglab14_1_2b/Granger Casuality/img/\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "(32, 32, 4)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for i in range(len(onlyfiles)):\n",
    "    data = Image.open(mypath + onlyfiles[i])\n",
    "    arr = np.array(data)\n",
    "    #arr = arr[:,:,0:3]\n",
    "    #result = np.zeros((32,32))\n",
    "    #toadd = np.zeros((32,32,4))\n",
    "    #for k in range(arr.shape[2]-1):\n",
    "    #    result[:arr[:,:,k].shape[0],:arr[:,:,k].shape[1]] = arr[:,:,k] \n",
    "    #    toadd[:,:,k] = result\n",
    "    X.append(arr)\n",
    "print(len(X))\n",
    "print(X[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/apple/Desktop/eeglab14_1_2b/participant_ratings.csv',\n",
    "                sep=r'\\s*,\\s*',engine = 'python', na_values = '?')\n",
    "df.dropna()\n",
    "Y_chart = pd.get_dummies(df, drop_first=True)\n",
    "Y = Y_chart['Arousal'].tolist()\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Y)):\n",
    "    if Y[i] < 5:\n",
    "        Y[i] = 0\n",
    "    else:\n",
    "        Y[i] = 1\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=(40/len(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1240, 32, 32, 4)\n",
      "(1240,)\n",
      "(40, 32, 32, 4)\n",
      "(40,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dataset have shape = (1240, 32, 32, 4)\n",
      "Image dataset has min/mean/std/max = 1.00/139.61/79.38/255.00\n",
      "\n",
      "Train label has shape = (1240,)\n",
      "Training label has min/mean/std/max = 0.00/0.59/0.49/1.00\n"
     ]
    }
   ],
   "source": [
    "print('Image dataset have shape =', X_train.shape)\n",
    "print('Image dataset has min/mean/std/max = %.2f/%.2f/%.2f/%.2f'%(X_train.min(),\n",
    "                        X_train.mean(), X_train.std(), X_train.max()))\n",
    "print('')\n",
    "print('Train label has shape =', y_train.shape)\n",
    "print('Training label has min/mean/std/max = %.2f/%.2f/%.2f/%.2f'%(y_train.min(),\n",
    "                        y_train.mean(), y_train.std(), y_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dataset have shape = (1240, 32, 32, 4)\n",
      "Image dataset has min/mean/std/max = 0.00/0.55/0.31/1.00\n",
      "\n",
      "Train label has shape = (1240,)\n",
      "Training label has min/mean/std/max = 0.00/0.59/0.49/1.00\n"
     ]
    }
   ],
   "source": [
    "def normalize_data(data): \n",
    "    data = data / data.max()\n",
    "    return data\n",
    "\n",
    "X_train = normalize_data(X_train)\n",
    "X_test = normalize_data(X_test)\n",
    "print('Image dataset have shape =', X_train.shape)\n",
    "print('Image dataset has min/mean/std/max = %.2f/%.2f/%.2f/%.2f'%(X_train.min(),\n",
    "                        X_train.mean(), X_train.std(), X_train.max()))\n",
    "print('')\n",
    "print('Train label has shape =', y_train.shape)\n",
    "print('Training label has min/mean/std/max = %.2f/%.2f/%.2f/%.2f'%(y_train.min(),\n",
    "                        y_train.mean(), y_train.std(), y_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    " [transforms.ToTensor(),\n",
    " transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "tensor_x = torch.stack([torch.Tensor(i) for i in X_train])\n",
    "tensor_y = torch.from_numpy(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = utils.TensorDataset(tensor_x,tensor_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = utils.DataLoader(trainset,  batch_size= 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x_test = torch.stack([torch.Tensor(i) for i in X_test])\n",
    "tensor_y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = utils.TensorDataset(tensor_x_test,tensor_y_test)\n",
    "testloader = utils.DataLoader(testset,  batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "classes = ('Positive', 'Negative')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 32, 32, 4])\n",
      "torch.Size([40, 4, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.size())\n",
    "images = images.permute(0, 3, 1, 2)\n",
    "print(images.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "rows = 1\n",
    "columns = 1\n",
    "fig=plt.figure()\n",
    "for i in range(1):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    img = images[i]\n",
    "    img = torchvision.transforms.ToPILImage()(img)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nkernal size 3*3\\nstep size = 1\\nmaximum pooled layer of 2*2\\nstep size = 2\\nlearning rate = 0.0001\\nbatch size = 63\\ndropout rate = 0.5\\nk-fold: 5-fold cross validation\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "kernal size 3*3\n",
    "step size = 1\n",
    "maximum pooled layer of 2*2\n",
    "step size = 2\n",
    "learning rate = 0.0001\n",
    "batch size = 63\n",
    "dropout rate = 0.5\n",
    "k-fold: 5-fold cross validation\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_classes = 2\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 10, 3, padding=1)\n",
    "        self.drop_rate = 0.4\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 3, padding=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(20*8*8, 100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(100, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 20 * 8 * 8)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(4, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2_bn): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (drop_out): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=576, out_features=96, bias=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=96, out_features=6, bias=True)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Linear(in_features=6, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 2\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 6, 3)\n",
    "        self.conv2_bn = nn.BatchNorm2d(20)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.drop_out = nn.BatchNorm2d(20)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(16*6*6, 96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(96, 6),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(6, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        #x = F.dropout2d(x, p=0.5)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.flat(x))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def flat(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num = 1\n",
    "        for s in size:\n",
    "            num *= s\n",
    "        return num\n",
    "    \n",
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with torch.no_grad():\\n    net = NeuralNet(model, criterion= nn.CrossEntropyLoss,\\n                max_epochs = args.epochs,\\n                lr = args.learning_rate,\\n                batch_size = 32,\\n                optimizer=optim.SGD,\\n                optimizer_momentum = 0.09,\\n                iterator_train__shuffle = True,\\n                iterator_train__num_workers = 4,\\n                iterator_valid__shuffle = True,\\n                iterator_valid__num_workers = 4,\\n                train_split= predefined_split(valid_dataset),\\n                callbacks= [ lr_scheduler, epoch_acc, checkpoint],\\n                device = 'cuda')\\nparams = {\\n        'lr':[0.01,0.02],\\n        'max_epochs':[10,20],\\n        'module_num_units': [10,20],\\n        }\\ngs = GridSearchCV(net, params,refit=False, cv=5, scoring='accuracy')\\ngs.fit(train_dataset, y=None)\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''with torch.no_grad():\n",
    "    net = NeuralNet(model, criterion= nn.CrossEntropyLoss,\n",
    "                max_epochs = args.epochs,\n",
    "                lr = args.learning_rate,\n",
    "                batch_size = 32,\n",
    "                optimizer=optim.SGD,\n",
    "                optimizer_momentum = 0.09,\n",
    "                iterator_train__shuffle = True,\n",
    "                iterator_train__num_workers = 4,\n",
    "                iterator_valid__shuffle = True,\n",
    "                iterator_valid__num_workers = 4,\n",
    "                train_split= predefined_split(valid_dataset),\n",
    "                callbacks= [ lr_scheduler, epoch_acc, checkpoint],\n",
    "                device = 'cuda')\n",
    "params = {\n",
    "        'lr':[0.01,0.02],\n",
    "        'max_epochs':[10,20],\n",
    "        'module_num_units': [10,20],\n",
    "        }\n",
    "gs = GridSearchCV(net, params,refit=False, cv=5, scoring='accuracy')\n",
    "gs.fit(train_dataset, y=None)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:    19] avg mini-batch loss: 0.692\n",
      "[epoch: 1, i:    19] avg mini-batch loss: 0.678\n",
      "[epoch: 2, i:    19] avg mini-batch loss: 0.681\n",
      "[epoch: 3, i:    19] avg mini-batch loss: 0.681\n",
      "[epoch: 4, i:    19] avg mini-batch loss: 0.676\n",
      "[epoch: 5, i:    19] avg mini-batch loss: 0.675\n",
      "[epoch: 6, i:    19] avg mini-batch loss: 0.673\n",
      "[epoch: 7, i:    19] avg mini-batch loss: 0.674\n",
      "[epoch: 8, i:    19] avg mini-batch loss: 0.676\n",
      "[epoch: 9, i:    19] avg mini-batch loss: 0.674\n",
      "[epoch: 10, i:    19] avg mini-batch loss: 0.676\n",
      "[epoch: 11, i:    19] avg mini-batch loss: 0.677\n",
      "[epoch: 12, i:    19] avg mini-batch loss: 0.671\n",
      "[epoch: 13, i:    19] avg mini-batch loss: 0.676\n",
      "[epoch: 14, i:    19] avg mini-batch loss: 0.674\n",
      "[epoch: 15, i:    19] avg mini-batch loss: 0.678\n",
      "[epoch: 16, i:    19] avg mini-batch loss: 0.676\n",
      "[epoch: 17, i:    19] avg mini-batch loss: 0.675\n",
      "[epoch: 18, i:    19] avg mini-batch loss: 0.677\n",
      "[epoch: 19, i:    19] avg mini-batch loss: 0.671\n",
      "[epoch: 20, i:    19] avg mini-batch loss: 0.674\n",
      "[epoch: 21, i:    19] avg mini-batch loss: 0.674\n",
      "[epoch: 22, i:    19] avg mini-batch loss: 0.675\n",
      "[epoch: 23, i:    19] avg mini-batch loss: 0.673\n",
      "[epoch: 24, i:    19] avg mini-batch loss: 0.672\n",
      "[epoch: 25, i:    19] avg mini-batch loss: 0.674\n",
      "[epoch: 26, i:    19] avg mini-batch loss: 0.673\n",
      "[epoch: 27, i:    19] avg mini-batch loss: 0.674\n",
      "[epoch: 28, i:    19] avg mini-batch loss: 0.672\n",
      "[epoch: 29, i:    19] avg mini-batch loss: 0.669\n",
      "[epoch: 30, i:    19] avg mini-batch loss: 0.677\n",
      "[epoch: 31, i:    19] avg mini-batch loss: 0.672\n",
      "[epoch: 32, i:    19] avg mini-batch loss: 0.670\n",
      "[epoch: 33, i:    19] avg mini-batch loss: 0.675\n",
      "[epoch: 34, i:    19] avg mini-batch loss: 0.672\n",
      "[epoch: 35, i:    19] avg mini-batch loss: 0.675\n",
      "[epoch: 36, i:    19] avg mini-batch loss: 0.670\n",
      "[epoch: 37, i:    19] avg mini-batch loss: 0.673\n",
      "[epoch: 38, i:    19] avg mini-batch loss: 0.676\n",
      "[epoch: 39, i:    19] avg mini-batch loss: 0.676\n",
      "[epoch: 40, i:    19] avg mini-batch loss: 0.677\n",
      "[epoch: 41, i:    19] avg mini-batch loss: 0.677\n",
      "[epoch: 42, i:    19] avg mini-batch loss: 0.677\n",
      "[epoch: 43, i:    19] avg mini-batch loss: 0.672\n",
      "[epoch: 44, i:    19] avg mini-batch loss: 0.670\n",
      "[epoch: 45, i:    19] avg mini-batch loss: 0.669\n",
      "[epoch: 46, i:    19] avg mini-batch loss: 0.676\n",
      "[epoch: 47, i:    19] avg mini-batch loss: 0.678\n",
      "[epoch: 48, i:    19] avg mini-batch loss: 0.672\n",
      "[epoch: 49, i:    19] avg mini-batch loss: 0.677\n",
      "[epoch: 50, i:    19] avg mini-batch loss: 0.672\n",
      "[epoch: 51, i:    19] avg mini-batch loss: 0.673\n",
      "[epoch: 52, i:    19] avg mini-batch loss: 0.673\n",
      "[epoch: 53, i:    19] avg mini-batch loss: 0.673\n",
      "[epoch: 54, i:    19] avg mini-batch loss: 0.672\n",
      "[epoch: 55, i:    19] avg mini-batch loss: 0.673\n",
      "[epoch: 56, i:    19] avg mini-batch loss: 0.677\n",
      "[epoch: 57, i:    19] avg mini-batch loss: 0.669\n",
      "[epoch: 58, i:    19] avg mini-batch loss: 0.672\n",
      "[epoch: 59, i:    19] avg mini-batch loss: 0.672\n",
      "[epoch: 60, i:    19] avg mini-batch loss: 0.676\n",
      "[epoch: 61, i:    19] avg mini-batch loss: 0.671\n",
      "[epoch: 62, i:    19] avg mini-batch loss: 0.672\n",
      "[epoch: 63, i:    19] avg mini-batch loss: 0.673\n",
      "[epoch: 64, i:    19] avg mini-batch loss: 0.673\n",
      "[epoch: 65, i:    19] avg mini-batch loss: 0.670\n",
      "[epoch: 66, i:    19] avg mini-batch loss: 0.675\n",
      "[epoch: 67, i:    19] avg mini-batch loss: 0.674\n",
      "[epoch: 68, i:    19] avg mini-batch loss: 0.675\n",
      "[epoch: 69, i:    19] avg mini-batch loss: 0.673\n",
      "[epoch: 70, i:    19] avg mini-batch loss: 0.672\n",
      "[epoch: 71, i:    19] avg mini-batch loss: 0.673\n",
      "[epoch: 72, i:    19] avg mini-batch loss: 0.675\n",
      "[epoch: 73, i:    19] avg mini-batch loss: 0.671\n",
      "[epoch: 74, i:    19] avg mini-batch loss: 0.672\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "#opt = torch.optim.Adam(net.parameters(), lr= 0.001)\n",
    "#opt = torch.optim.Adamax(net.parameters(), lr=0.005)\n",
    "#opt = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "opt = torch.optim.Adagrad(net.parameters(), lr=0.005)\n",
    "avg_losses = [] \n",
    "epochs = 1000 \n",
    "print_freq = 20\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1:\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(total)\n",
    "print('Accuracy of the network on the 40 test images: %d %%' % (100 * correct / total))\n",
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        one = torch.tensor(1, dtype=torch.float, device=device)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(40):\n",
    "            label = labels[i]\n",
    "            labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "for i in range(2):\n",
    "    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(total)\n",
    "print('Accuracy of the network on the 1240 training images: %d %%' % (100 * correct / total))\n",
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        one = torch.tensor(1, dtype=torch.float, device=device)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(40):\n",
    "            label = labels[i]\n",
    "            labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "for i in range(2):\n",
    "    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "kernal size 3*3\n",
    "step size = 1\n",
    "maximum pooled layer of 2*2\n",
    "step size = 2\n",
    "learning rate = 0.0001\n",
    "batch size = 63\n",
    "dropout rate = 0.5\n",
    "k-fold: 5-fold cross validation\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
